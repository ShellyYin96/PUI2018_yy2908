{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is designed to pratice with both geopandas and census data.  This is a simple spatial exploratory analysis of census data. The goal is to assess wheather the location of the [linkNYC](https://www.link.nyc/) access points is optimal to guarantee a more \"democratic\" access to the internet.\n",
    "\n",
    "## THE CENSUS: \n",
    "make sure you do the assigned reading about the census. Census data are a national trasure! The census is a survey  designed to collect data on every person that lives in the USA every 10 years. The Census Bureau collects data, designes aggregation areas (the census blocks and tracts for example, which are designed for maximal homogeneity across all features). However, the Census Bureau does not only collect and aggregate data every 10 years, it also collects the America Community Survey every 5 years, and some more restricted survey on an annual bases. The 1 year survey data is what we will use: https://www.census.gov/services/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PUMA : Public Use Microdata Area\n",
    "Census geographies that are not specific political entities (i.e. states, counties etc) are designed for homogeneity so that aggregating the data over the whole area will leat to an estimat (mean or median for example) with minimal variance. Among these geographies are the _census tracts_, _census blocks_, and _Public Use Microdata Areas (PUMAs)_. PUMAs are geographical areas designed to aggregate census data. Each PUMA contains at least 100,000 people. PUMAs do not overlap, and are contained within a single state. \n",
    "\n",
    "## 1.1 download the NYC  Public Use Microdata Areas (PUMA) geometry fron the NYC Open Data API and read it in with geopandas\n",
    "\n",
    "https://data.cityofnewyork.us/Housing-Development/Public-Use-Microdata-Areas-PUMA-/cwiz-gcty/data\n",
    "\n",
    "download it as a shape file. When you download a shapefile you actually download a zipped folder which contains the shape file and other files that are necessary to read the shape file in. You can do that with the urllib library in python 3 with the function \n",
    "\n",
    "        urllib.request.urlretrieve(url, \"file.gz\")\n",
    "\n",
    "then unpack the data into your PUIdata directory, then read it in with geopandas, reading in the shape file with the function \n",
    "\n",
    "        geopandas.GeoDataFrame.from_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUMAs were first created for the 1990 Census. PUMAs are identified by a specific 4 numbers id. In the shapefile foung at data.cityofnewyork.us the id is named \"puma\" and it is in fact a 4 digits number. You can read it in as an integer (although of course it is a categorical variable inherently!). Often the PUMA id is found in conjunction with the 3 numbers state id, leading to a 7 numbers identification. The id for the State of NY is 036. See this link for more: https://www.census.gov/geo/reference/geoidentifiers.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "from fiona.crs import from_epsg\n",
    "import requests\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://data.cityofnewyork.us/api/geospatial/cwiz-gcty?method=export&format=Shapefile', \"file.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PUIDATA']='%s/PUIdata'%os.getenv('HOME')\n",
    "os.getenv('PUIDATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv file.gz $PUIDATA\n",
    "!unzip -o $PUIDATA/file.gz -d $PUIDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumashp = gpd.GeoDataFrame.from_file(os.getenv('PUIDATA') + '/' + 'geo_export_08217aa3-06bc-4760-96c6-ecd1d552ff8c.shp')\n",
    "pumashp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumashp.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  plot the PUMA NYC regions\n",
    "\n",
    "you can use the geopandas function plot. \n",
    "What is the appropriate plot to just show the shape of the PUMA regions? A choropleth could be ok, but it is better to just draw the contours of the region, since we do not want to highlight some regions over others by color choices. \n",
    "\n",
    "(Note: a while ago I wrote a quick function that plots choropleths and maps of NYC specifically putting legends and colorbars in the empty spaces taking advantage of the shape of the city and you are welcome to use it: https://github.com/fedhere/choroplethNYC. BUT YOU DO NOT HAVE TO)\n",
    "\n",
    "If you want to learn how to install a package from within the notebook look here https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "Your map should look someting like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the built in plot function as pumashp.plot(pumashp....)\n",
    "# using a choroplethNYC function that FBB wrote\n",
    "import choroplethNYC as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pumashp.plot(figsize=(12,12),color=\"white\", edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## caption:\n",
    "The plot shows contours of NYC PUMA regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. American Fact Finder data on percentage of houses with broadband internet access\n",
    "\n",
    "Download the table of data from 2016 for NYC: you want to obtain data on broadband access (percentage of households with broadband access) at the PUMA (Public Use Microdata Area) geographical area level. \n",
    "\n",
    "This may be tricky. But familiarizing with Census and American Commjunity Survey data is super important for urban science. Here you should use the API, but you should also practice interacting with the website: \n",
    "\n",
    "You  should read the datta in thgouth the API but also download the data manually and compare the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Get data with American Fact Finder (AFF) API: \n",
    "you can request an API key, although I think it is not necessary for this search (but it is good to practice). Obtain a key and save it into a python file. DO NOT UPLOAD THE FILE CONTAINING THE API KEY TO GITHUB. Keep your API keys private. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in my api key saved in censusAPI.py as\n",
    "#myAPI = 'XXXXXXXXXXXXXXX'\n",
    "from censusAPI import myAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need help with the API you can ask questions in this [Gitter channel](https://gitter.im/uscensusbureau/general) (I did myself to design this homework!) \n",
    "The internet subscription by household is data surveyd annually. You can find out what are the features that you can query through the API for the annually surveyed data at this url: https://api.census.gov/data/2016/acs/acs1/variables.json\n",
    "\n",
    "read it in as a jason file - you can do it with pandas (but it is slow). You can do it as I do below with the request package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in in the variables available. the info you need is in the 1year ACS data\n",
    "url = \"https://api.census.gov/data/2016/acs/acs1/variables.json\"\n",
    "resp = requests.request('GET', url)\n",
    "aff1y = json.loads(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning things into arrays to enable broadcasting\n",
    "#Python3\n",
    "affkeys = np.array(list(aff1y['variables'].keys()))\n",
    "#Python2\n",
    "#affkeys = np.array(aff1y['variables'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need two features: total number of households, and number of households with broadband access\n",
    "\n",
    "Broadband access is one of the B28002 keys. However there are many of those! Each variable has several observations associated to it: B28002 is all internet access data (broadband, dial up, none, ...), and for each of those you have the estimate (count, mean, median, percentage...), the margin of errors, and annotations. Those are identified by an appendix that begins with \"_\". Look carefully at the line of code below and its output. I am creating a list that contains the keys of the dictionary I created from the json file for all B28002 observations (rows that start with \"B28002\") that include the word \"Broadband\" in the description.\n",
    "\n",
    "The syntax is a lost comprehension:\n",
    "\n",
    "    as = \\[a for a in listOfAs\\] \n",
    "\n",
    "is simply a compact way to write\n",
    "\n",
    "    as = \\[\\]\n",
    "    for a in listOfAs:\n",
    "        as.append(a)\n",
    "\n",
    "with an if statement tha selects broadband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting variables of B28002 that contain \"broadband\"\n",
    "[(k, aff1y['variables'][k]['label'])  for k in affkeys if k.startswith (\"B28002\") and \n",
    " 'Broadband' in aff1y['variables'][k]['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chose the appropriate variable and extract the relevant data: below I select the variable containing the number of household per PUMA. The \"all\" variable, which tells you what is the total number of units queried (households here) is generally stored in the \\_001 variable (B28002_001 in this case). \"E\" stands for *estimate*. M stands for *margin of error*, EA *estimate annotations*, MA *margin of error annotations*. You want the estimate for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword for the number of households\n",
    "keyNhouseholds = 'B28002_001E'\n",
    "aff1y['variables'][keyNhouseholds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, choose the fractions of houses with any \"With an Internet subscription!!Broadband of any type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword for the number of households with broarband access\n",
    "keyNBB = 'B28002_004E'\n",
    "aff1y['variables'][keyNBB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the variable names use them to extract the relevant data with the ACS API. \n",
    "\n",
    "The API url is as follows:\n",
    "\n",
    "root: https://api.census.gov/data/2016/acs/acs1\n",
    "\n",
    "action: get=\\[variable Name\\],NAME\n",
    "\n",
    "geometry: for=\\[geometry\\]:\\[desired geometry values\\]in=\\[larger geometry\\]:\\[desired larger geometry values\\]\n",
    "\n",
    "API key: key:\\[api key\\]\n",
    "\n",
    "the URL is constructed as root?action&geometry&key\n",
    "\n",
    "Note that it took me a long time to figure out how to request the right geometry: in the url I write below the geometry is \"public%20use%20microdata%20area\" where %20 is the character for ' ' (space) in a url, and \":\\*\" means all PUMAS\n",
    "\n",
    "state:36 is New York State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the broadband access number of households\n",
    "url = \"https://api.census.gov/data/2016/acs/acs1?get=\" + keyNBB +\\\n",
    "\",NAME&for=public%20use%20microdata%20area:*&in=state:36&key=\" + myAPI\n",
    "resp = requests.request('GET', url).content\n",
    "pumaBB = pd.read_csv(io.StringIO(resp.decode('utf-8').replace('[','').replace(']','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumaBB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the first row of the table: the total number of households\n",
    "url = \"https://api.census.gov/data/2016/acs/acs1?get=\" + keyNhouseholds +\\\n",
    "\",NAME&for=public%20use%20microdata%20area:*&in=state:36&key=\" + myAPI\n",
    "resp = requests.request('GET', url).content\n",
    "pumaPP = pd.read_csv(io.StringIO(resp.decode('utf-8').replace('[','').replace(']','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumaPP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 generate a feature for the percentage of households with broadband access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pumaBB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumaBB = pumaBB.merge(pumaPP, on='Name')\n",
    "pumaBB = pumaBB.drop(['Unnamed: 4'], axis = 1,inplace=True)\n",
    "pumaBB['pcBB'] = pumaBB['B28002_004E'] / pumaBB['B28002_001E'] * 100\n",
    "pumaBB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Now compare it with the AFF published \"percentage of households with broadband access\" which is a feature available through their website but that cannot be downloaded directly from the API. It compiled by AFF the same way we did above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually you can download the GCT Geographic Comparison Tables. Download the GCT table that contains the percentage of households in each PUMA with broadband internet subscription as follows:\n",
    "\n",
    "https://factfinder.census.gov/ -> Advanced Search -> Show me all \n",
    "\n",
    "    - Topics: Product Type -> Geography Comparison Table \n",
    "    \n",
    "select the PERCENT OF HOUSEHOLDS WITH A BROADBAND INTERNET SUBSCRIPTION  at our geography granularity (PUMA) and click Download below. This will generate the table on the fly and you can click again on Download.\n",
    "\n",
    "Move the file into the PUIdata directory, unzip it and load it with pandas.\n",
    "    \n",
    "Your table shold be labeled as: GCT2801\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more hurdle: you need to move this data to jupyterhub/compute, and the link is not something you can copy and paste!\n",
    "Two solutions:\n",
    "\n",
    "-easier but less preferible: upload the table you downloaded locally to github AWS, or else where, and download it from compute\n",
    "\n",
    "-otherwise, learn how to do it with sftp from your local terminal: these are the steps\n",
    "\n",
    "1. ON YOUR LOCAL MACHINE where you downloaded the table (which was downloaded as aff_download.zip for me in my ~/Downloads folder) type the following *sftp* command\n",
    "        \n",
    "        fbianco@Federicas-MacBook-Air:~$ sftp fbianco@staging.cusp.nyu.edu\n",
    "    \n",
    "2. Input your password\n",
    "            \n",
    "        Password: \n",
    "        Connected to staging.cusp.nyu.edu.\n",
    "\n",
    "   This should open an sftp promopt (you should see sftp> at the beginning of the line). Go to the PUI directory (for me /home/fbianco/PUIdata)\n",
    "    \n",
    "        sftp> cd /home/fbianco/PUIdata             \n",
    "        \n",
    "3. Use the sftp command _put_ to copy the file from the local to the remote machine to compute, making sure you use the full path (unless the file was downloaded in your local directory in the local machine)\n",
    "    \n",
    "        sftp> put /Users/fbianco/Downloads/aff_download.zip\n",
    "        \n",
    "You should see:\n",
    "\n",
    "            Uploading /Users/fbianco/Downloads/aff_download.zip to ....\n",
    "\n",
    "And the file can be unzipped and read into your code with pandas. I leave the read in line below for your convenience, those are the only variables you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbpc = pd.read_csv(os.getenv(\"PUIDATA\") + \"/ACS_16_1YR_GCT2801.ST50_with_ann.csv\",\n",
    "            usecols=[\"GCT_STUB.target-geo-id2\",\"HC01\",\"HC02\"])\n",
    "\n",
    "bbpc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice 2 things:\n",
    "    - that the Target Geo Id2: GCT_STUB.target-geo-id2 is a 7 digits number. See my comment above about it. \n",
    "    - that there is a weird double header. You can deal with it in one of 2 ways: either reread the file in skipping one row and using the second row as header, or remove that row (for example with bbpc.drop(0, inplace=True)) but also then you should check the *type* of your GCT_STUB.target-geo-id2 feature! The fact that pandas had to read in a column with nombers and characters forced it to read it as a string, and you need to convert it (.astype(int)) to merge it easily with the API acquired data.\n",
    "    \n",
    "In the API dataframe the PUMA id was a 4 digit number. If you have them both as integers and you remove the initial three digits (for example by subtracting 360000 from each value wich you can do as bbpc[\"gid\"] = bbpc.gid - 3600000 then you can merge on the puma id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbpc.columns = ['gid', 'HC01', 'HC02']\n",
    "bbpc = bbpc.drop([0])\n",
    "bbpc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bbpc['gid'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbpc.gid = bbpc.gid.astype(str)\n",
    "type(bbpc['gid'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbpc.gid = bbpc.gid.str[-4:]\n",
    "bbpc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbpc.gid = bbpc.gid.astype(int)\n",
    "type(bbpc['gid'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check that the percentage of households with broadband you generated and the one you red in from the table you downloaded manually are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ = pumaBB.merge(bbpc, right_on=\"gid\", \n",
    "                     left_on=\"public use microdata area\")[[\"pcBB\", \"HC01\"]]\n",
    "diff_[\"diff\"] = np.abs(diff_[\"pcBB\"] - diff_[\"HC01\"].astype(float))\n",
    "diff_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the maximum difference should only be a few percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Plot a choropleth of NYC broadband access \n",
    "## 3.1 Merge with the puma geodataframe and plot a choropleth of the percentage of households with broadband access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumashp.puma = pumashp.puma.astype(int)\n",
    "pumaBB['public use microdata area'] = pumaBB['public use microdata area'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumashp = pumashp.merge(pumaBB,left_on='puma',right_on='public use microdata area')\n",
    "pumashp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choropleth of the percentage of internet axcess\n",
    "cp.choroplethNYC(pumashp, column='pcBB', edgecolor='w');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# caption:\n",
    "The choropleth of NYC households with broadband access "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LinkNYC: assess whether the locations of the linkNYC stations are supplying internet where it is needed.\n",
    "\n",
    "acquire the linkNYC locations and prepare them into a dataframe\n",
    "read it in from the HW7_fb55 folder in :\n",
    "https://github.com/fedhere/PUI2018_fb55\n",
    "\n",
    "Notice that you can also get a linkNYC locations shapefile from NYC open data, as I did to generate this, but I want you do to do some extra coordinates gymnastics for practice so use the one I provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC = pd.read_csv('https://raw.githubusercontent.com/fedhere/PUI2018_fb55/master/HW7_fb55/linkNYClocations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine long lat into a column like you did in the lab to greate a \"geometry\" column for the dataframe, then convert the dataframe into a GeoDataFrame _linkNYC_ and set native coordinates  frame to lat/lon as you did in the lab\n",
    "    linkNYC.crs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "# combine lat and lon to one column\n",
    "linkNYC['lonlat']=list(zip(linkNYC.longitude,linkNYC.latitude))\n",
    "\n",
    "# Create Point Geometry for based on lonlat column\n",
    "linkNYC['geometry']=linkNYC[['lonlat']].applymap(lambda x:shapely.geometry.Point(x))\n",
    "linkNYC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC = gpd.GeoDataFrame(linkNYC)\n",
    "\n",
    "from fiona.crs import from_epsg\n",
    "linkNYC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC.crs = from_epsg(4326)\n",
    "linkNYC.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the location of the linkNYC stations on top of a choropleth of broadband access percentage in *5 equal intervals*\n",
    "\n",
    "I have also color coded the station by how long ago they have been installed, using the date datetime variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax,bar = cp.choroplethNYC(pumashp, scheme=\"Equal_interval\", k=5, column=\"pcBB\", cmap=\"bone\", edgecolor=\"black\", cb=False)\n",
    "linkNYC.plot(column='date_link_', cmap='Reds', alpha=1, lw=0.1, ax=ax, markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Find the number of linkNYC locations per person by PUMA\n",
    "\n",
    "## 5.1 with the AFF API from ACS get the total population by puma and merge it into your GeoDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the ACS population by  variable is B00001_001E, and of course your geometry is PUMA, _public%20use%20microdata%20area:*_, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyPOP = 'B00001_001E'\n",
    "aff1y['variables'][keyPOP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.census.gov/data/2016/acs/acs1?get=\" + keyPop +\\\n",
    "\",NAME&for=public%20use%20microdata%20area:*&in=state:36&key=\" + myAPI\n",
    "resp = requests.request('GET', url).content\n",
    "pumaPop = pd.read_csv(io.StringIO(resp.decode('utf-8').replace('[','').replace(']','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumaPop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 find the number of station per 100 people per PUMA\n",
    "**Important** you can do this with an sjoin(). But sjoin() should not be used with lat/lon coordinates cause they are **not \"flat coordinates\"**. Since spattial joins are done in cartesian geometry the only coordinate systems suitable to spatial joins are flat coordinate plane systems, which in the NYC area is 2263. So before you proceed to the sjoin you have to convert both pumashp and linkNYC to 2263 (for example with GeoDataFrame method .to_crs(epsg=...) )\n",
    "\n",
    "\n",
    "(A note: You can also do this by hand by asking for each point if it is in any of the regions, for example in a for loop. But that is escruciatingly slow, unless you get smart about your requests (for example for a given PUMA not asking if the coordinates of a point are very different from the center of a PUMA). With the new version of geopandas this is not needed, cause the sjoin is fast, but with the older versions this was sometimes better) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert coordinates for pumashp and linkNYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a geodataframe with a spatial join and use groupby to count the number of linkNYC in each PUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumashp = pumashp.to_crs(epsg=2263)\n",
    "linkNYC = linkNYC.to_crs(epsg=2263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumashp.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkNYC.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linkpp = gpd.sjoin(..).groupby(...)\n",
    "#inkpp.head()\n",
    "linkpp = gpd.sjoin(pumashp,linkNYC).groupby('puma').count()[['link_site']]\n",
    "linkpp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkpp.reset_index(inplace=True)\n",
    "linkpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally merge back into the pumashp. Make sure you use the correct scheme for merging: you want to have _all_ the PUMAs, not only the ones that have linkNYC in them in the final geoDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumashplc = pumashp.merge(linkpp,left_on='puma',right_on='puma',how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumashplc['linkNYCp100p'] = (pumashplc['link_site']/pumaPop['B00001_001E'])*100\n",
    "pumashplc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linkNYC per 100 inhabitants\n",
    "pumashplc[\"linkNYCp100p\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# at last, plot the linkNYC locations on top of a choropleth of number of stations per 100 people  in 10 equal intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax,bar = cp.choroplethNYC(pumashplc, scheme=\"Equal_interval\", k=10, column=\"linkNYCp100p\", cmap=\"bone\", edgecolor=\"black\", cb=False)\n",
    "linkNYC.plot(c=linkNYC.date_link_,cmap='Reds',ax=ax,markersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra credit 1: \n",
    "How would you enhance the visibility of this map? you are working with numbers that are mostly very low, and reside near each other, and them have a few much higher value points. Notice that here the high values are interesting though and should not be thrown away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.hist(pumashplc[\"linkNYCpp\"], bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax,bar = cp.choroplethNYC(pumashplc, scheme=\"Quantiles\", k=10, column=\"linkNYCp100p\", cmap=\"bone\", edgecolor=\"black\", cb=False)\n",
    "linkNYC.plot(c=linkNYC.date_link_,cmap='Reds',ax=ax,markersize=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shelly",
   "language": "python",
   "name": "shelly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
